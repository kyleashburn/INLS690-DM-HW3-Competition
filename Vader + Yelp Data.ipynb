{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b774e4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4eb2ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ashbu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "from scipy.sparse import hstack, coo_matrix\n",
    "import sklearn\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7d9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function: write out prediction values into a csv format file\n",
    "# params:\n",
    "#     df: dataframe, where each row is a test example, with column 'id' as data id\n",
    "#     pred: a list or 1-d array of prediction values\n",
    "#     filepath: the output file path\n",
    "# return:\n",
    "#     None\n",
    "\n",
    "def write_test_prediction(df, pred, filepath):\n",
    "    with open(filepath, 'w') as outfile:\n",
    "        outfile.write('{},{}\\n'.format('id', 'label'))\n",
    "        for index, row in df.iterrows():\n",
    "            outfile.write('{},{}\\n'.format(row['id'], pred[index]))\n",
    "    print (len(df), 'predictions are written to', filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b03fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv('./train.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e5c3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_data = pd.read_csv('./extra data/yelp_prepped.tsv', sep = '\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1904943",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_data.rename(columns={0: \"label\",1: \"review\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "080ece7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = dataframe.append(yelp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f107afd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5046918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 32.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_combined[\"rating\"] = data_combined[\"review\"].apply(analyzer.polarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24225375",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = pd.concat([data_combined.drop(['rating'], axis=1), data_combined['rating'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03a74f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training set size: 30828\n",
      "validation set size: 4979\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.8 # 80% for training, 20% for validation\n",
    "random_seed = 100\n",
    "\n",
    "train_dataframe = data_combined.sample(frac=train_ratio, random_state=random_seed)\n",
    "valid_dataframe = data_combined.drop(train_dataframe.index)\n",
    "print('training set size:', len(train_dataframe))\n",
    "print('validation set size:', len(valid_dataframe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d737e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the test dataframe\n",
    "test_dataframe = pd.read_csv('./test.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b4fec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# runnign Vader on the test_dataframe\n",
    "test_dataframe[\"rating\"] = test_dataframe[\"review\"].apply(analyzer.polarity_scores)\n",
    "test_dataframe = pd.concat([test_dataframe.drop(['rating'], axis=1), test_dataframe['rating'].apply(pd.Series)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78f189b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(ngram_range=(1, 3), stop_words={'english'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words={\"english\"},ngram_range=(1,3))\n",
    "vectorizer.fit(train_dataframe[\"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c0545ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8.74 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_X = vectorizer.transform(train_dataframe[\"review\"])\n",
    "valid_X = vectorizer.transform(valid_dataframe[\"review\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f22ada3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_train = coo_matrix(train_dataframe[[\"neg\",\"neu\",\"pos\",\"compound\"]])\n",
    "sparse_valid = coo_matrix(valid_dataframe[[\"neg\",\"neu\",\"pos\",\"compound\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7614f033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking the scores onto the sparse matrix\n",
    "train_X = hstack((sparse_train, train_X))\n",
    "valid_X =  hstack((sparse_valid, valid_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f1f36bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking test dataframe\n",
    "sparse_test = coo_matrix(test_dataframe[[\"neg\",\"neu\",\"pos\",\"compound\"]])\n",
    "test_X = vectorizer.transform(test_dataframe[\"review\"])\n",
    "test_X = hstack((sparse_test, test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "254e4a17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, solver='liblinear')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y = train_dataframe[\"label\"]\n",
    "model = LogisticRegression(C = 1, solver='liblinear')\n",
    "model.fit(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cada369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression, accuracy on training set: 0.9215323731672506\n"
     ]
    }
   ],
   "source": [
    "train_Y_hat = model.predict(train_X)\n",
    "accuracy = accuracy_score(train_dataframe[\"label\"], train_Y_hat)\n",
    "print ('Logistic regression, accuracy on training set:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f6e006b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression, accuracy on validation set: 0.8656356698132155\n"
     ]
    }
   ],
   "source": [
    "valid_Y_hat = model.predict(valid_X)\n",
    "accuracy = accuracy_score(valid_dataframe[\"label\"], valid_Y_hat)\n",
    "print ('Logistic regression, accuracy on validation set:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7192a60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30828, 2715105)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a93b21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc31d80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_train_Y = data_combined['label']\n",
    "all_train_X = vectorizer.transform(data_combined['review'])\n",
    "all_train_sparse = coo_matrix(data_combined[[\"neg\",\"neu\",\"pos\",\"compound\"]])\n",
    "all_train_X = hstack((all_train_sparse, all_train_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "327847b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e4b0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eeae40b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.53979229927063\n"
     ]
    }
   ],
   "source": [
    "print(run_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f56e436",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d27e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ba386d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000 predictions are written to ./logistic_regression_vader_yelp.csv\n"
     ]
    }
   ],
   "source": [
    "model.fit(all_train_X, all_train_Y)\n",
    "test_Y_hat = model.predict(test_X)\n",
    "# write_test_prediction(test_dataframe, test_Y_hat, './logistic_regression_vader_yelp.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
